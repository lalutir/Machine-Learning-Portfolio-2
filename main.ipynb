{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqUwtJsjvd4l"
   },
   "source": [
    "# 0. Loading packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kFaPROuvd4m"
   },
   "source": [
    "Uncomment cell below if not all necessary packages are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rRmh73vvd4m"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install pandas\n",
    "# %pip install seaborn\n",
    "# %pip install scikit-learn\n",
    "# %pip install missingno\n",
    "# %pip install imblearn\n",
    "# %pip install xgboost\n",
    "# %pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BH3XRBRmvd4n"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import functions as fc\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIgLqmrBvd4n"
   },
   "source": [
    "# 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEImBTAcvd4n"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Datasets/train.csv')\n",
    "test_data = pd.read_csv('Datasets/test.csv')\n",
    "\n",
    "test_data_pred_col = list(test_data['date_hour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEKSzVoYvd4o"
   },
   "source": [
    "# 2. Inspecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRu9mGDtvd4o"
   },
   "source": [
    "## 2.1 Showing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bmc8YU8lvd4o",
    "outputId": "795fd12e-6ae2-48e0-f038-e7109df6265b"
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey0NfcCAvd4o",
    "outputId": "bf77b67e-617b-485c-c1c7-6fb20b3aeadd"
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "gWGJ2mocvd4p",
    "outputId": "959ca840-1332-4af8-d06c-1ad6811d73a2"
   },
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwDgCcErvd4p"
   },
   "source": [
    "The datasets contains no missing data.\n",
    "\n",
    "The columns in the dataset are predominantly of data types `int` or `float`, except for the `date_hour` column, which is of type `object`. This column will need to be converted to the `datetime` format for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RxCr4rOvd4p"
   },
   "source": [
    "## 2.2 Inspecting individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vieSwSXjvd4p"
   },
   "outputs": [],
   "source": [
    "cols = ['holiday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'cnt']\n",
    "\n",
    "dv_train = fc.DataVisualizer(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "Odt37G7Mvd4p",
    "outputId": "b395aaff-6f7a-4b04-b68d-47d39c048217"
   },
   "outputs": [],
   "source": [
    "dv_train.plot_distribution(cols, 'train_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wPQiCVivd4p"
   },
   "source": [
    "1. **Countplot for `holiday`:**\n",
    "    - **Majority of entries are non-holidays**: The count for `0` (non-holidays) is significantly higher than `1` (holidays), indicating that most of the data represents regular working or non-holiday days.\n",
    "2. **Countplot for `weathersit`:**\n",
    "    - **Category 1 dominates**: Most observations fall into category `1`, representing favorable or clear weather.\n",
    "    - **Category 2 and 3 are less common**: These represent moderate or less favorable weather conditions.\n",
    "    - **Category 4 is absent**: These imply extreme weather conditions are not present in the dataset.\n",
    "3. **Countplot for `temp`:**\n",
    "    - This column is normally distributed.\n",
    "4. **Countplot for `atemp`:**\n",
    "    - This column is normally distributed.\n",
    "5. **Countplot for `hum`:**\n",
    "    - This column is left skewed.\n",
    "6. **Countplot for `windspeed`:**\n",
    "    - This column is right skewed.\n",
    "7. **Countplot for `cnt`:**\n",
    "    - Most of the amounts for cnt are nearer to zero, indicating that higher amounts for `cnt` are preserved for specific occassions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFr3jl5ivd4q"
   },
   "source": [
    "## 2.3 Relationships between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "collapsed": true,
    "id": "mNxMgigqvd4q",
    "outputId": "f4495514-da39-4bba-8959-0a5aa5d15150"
   },
   "outputs": [],
   "source": [
    "dv_train.plot_correlation('train_data', method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mLUetRUvd4q"
   },
   "source": [
    "The target variable `cnt` exhibits the following correlations with the other features in the dataset:\n",
    "\n",
    "1. **`temp` (Temperature)**:\n",
    "   - Correlation: **0.41** (moderate positive)\n",
    "   - Interpretation: As temperature increases, the count of rentals tends to increase. This suggests that warmer weather is favorable for usage.\n",
    "\n",
    "2. **`atemp` (Feels-like Temperature)**:\n",
    "   - Correlation: **0.4** (moderate positive)\n",
    "   - Interpretation: Similar to `temp`, higher feels-like temperatures are associated with more rentals. Since `temp` and `atemp` are highly correlated with each other, their impact on `cnt` is quite similar.\n",
    "\n",
    "3. **`hum` (Humidity)**:\n",
    "   - Correlation: **-0.33** (moderate negative)\n",
    "   - Interpretation: Higher humidity levels are associated with a decrease in rentals. This indicates that humid weather may discourage people from renting.\n",
    "\n",
    "4. **`windspeed`**:\n",
    "   - Correlation: **0.097** (weak positive)\n",
    "   - Interpretation: Windspeed shows a very weak positive correlation with rentals. This suggests that windspeed has a minimal linear relationship with the count of rentals.\n",
    "\n",
    "5. **`weathersit` (Weather Situation)**:\n",
    "   - Correlation: **-0.14** (weak negative)\n",
    "   - Interpretation: Since this column is a column consisting of four classes, a pearson correlation coefficient is not the best way to figure out relations.\n",
    "\n",
    "6. **`holiday`**:\n",
    "   - Correlation: **-0.027** (very weak negative)\n",
    "   - Interpretation: The correlation between holidays and rentals is negligible, indicating that the number of rentals is not significantly affected by whether it is a holiday. Although, here again it is a column consisting of two classes, therefore, a pearson correlation coefficient is not the best way to find out relations.\n",
    "\n",
    "**Summary:**\n",
    "- The most significant predictors of `cnt` are `temp` (0.41), `atemp` (0.4), and `hum` (-0.33), as these exhibit moderate correlations.\n",
    "   - Since `temp` and `atemp` have a high correlation towards eachothter (0.99), one of them can be rendered negligible.\n",
    "- Features such as `windspeed`, `weathersit`, and `holiday` show weak or negligible correlations, indicating they may have limited linear influence on the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBdyJnfFvd4q"
   },
   "source": [
    "## 2.4 Inspecting trends, and seasonal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Series Decomposition**\n",
    "\n",
    "In time series analysis, we assume an additive decomposition model where the data can be expressed as:\n",
    "\n",
    "$$y_t = S_t + T_t + R_t$$\n",
    "\n",
    "Where:\n",
    "- **$y_t$**: The observed data at time $t$  \n",
    "- **$S_t$**: The seasonal component at time $t$  \n",
    "- **$T_t$**: The trend component at time $t$  \n",
    "- **$R_t$**: The residual (or irregular) component at time $t$  \n",
    "*(Hyndman & Athanasopoulos, 2018)*  \n",
    "\n",
    "\n",
    "**Insights from Decomposition Components**\n",
    "\n",
    "Decomposing the time series into its primary components provides valuable insights:\n",
    "\n",
    "1. **Trend**:  \n",
    "   The trend represents the long-term movement in the data. It reveals whether the overall direction of the data is increasing, decreasing, or stable. Short-term fluctuations are ignored as they may result from noise or temporary anomalies.\n",
    "\n",
    "2. **Seasonality**:  \n",
    "   The seasonality captures periodically repeating patterns in the data. These patterns occur at consistent intervals, such as daily, weekly, or annually, and reflect regular cyclical behavior.\n",
    "\n",
    "3. **Residuals**:  \n",
    "   The residuals represent the irregular component of the data. These are deviations that cannot be explained by either the trend or the seasonality, such as unexpected peaks or outliers.  \n",
    "   *(Dey, 2024)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tmdj-xXyvd4q"
   },
   "outputs": [],
   "source": [
    "train_dc = fc.TimeSeriesDecomposer(train_data['cnt'], period=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_lWophIvd4q"
   },
   "outputs": [],
   "source": [
    "trend, seasonal, residual = train_dc.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "collapsed": true,
    "id": "DiXUcewsvd4q",
    "outputId": "144f626a-7e95-4f32-a7fc-69883848383c"
   },
   "outputs": [],
   "source": [
    "train_dc.plot_decomposition(trend, seasonal, residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql3ZpAcovd4r"
   },
   "source": [
    "The plot above does not clearly reveal a seasonally repeating pattern, it is currently commented out due to runtime. This is likely due to the extensive amount of data, as it encompasses hourly observations over a two-year period. To facilitate the identification of seasonal patterns, a new decomposition will be performed on a subset comprising one-thirtysecond of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GL0fzogVvd4r"
   },
   "outputs": [],
   "source": [
    "train_dc_1 = fc.TimeSeriesDecomposer(train_data.iloc[:int(len(train_data)/32), :]['cnt'], period=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lofUC7Cmvd4r"
   },
   "outputs": [],
   "source": [
    "trend, seasonal, residual = train_dc_1.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 936
    },
    "collapsed": true,
    "id": "1Jb48k5gvd4r",
    "outputId": "cb3f36ba-e714-48b1-e938-5c07d2c2d78c"
   },
   "outputs": [],
   "source": [
    "train_dc_1.plot_decomposition(trend, seasonal, residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNu9yVVXvd4r"
   },
   "source": [
    "The plot above indicates a distinct **seasonal pattern** with a periodicity of approximately **one day**, suggesting a temporal influence on the **`cnt`** variable. However, there is no apparent trend in the data, which suggests that the dataset may already be stationary. This assumption will be further tested using the **Augmented Dickey-Fuller (ADF) test** in subsequent analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZ42htSgvd4r"
   },
   "source": [
    "## 2.5 Inspecting time specific relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the presence of daily seasonal patterns, visualizations will be created using different time elements (e.g., hour, day, week) on the x-axis and the target variable on the y-axis. These plots will help to identify and observe visible trends or repeating patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRnUssqSvd4r"
   },
   "outputs": [],
   "source": [
    "train_data = fc.create_timeseries_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CHSPFJBvd4r",
    "outputId": "dfca12fc-5325-4a4f-9d81-047271450d16"
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "JZqe7wrkvd4r",
    "outputId": "ba7850e0-3b6d-4fb4-85dc-d6d3d8f1f049"
   },
   "outputs": [],
   "source": [
    "cols = ['year', 'month', 'week', 'day', 'hour', 'day_of_week']\n",
    "\n",
    "for col in cols:\n",
    "    dv_train.lineplot(x=col, y='cnt', title=f'{col} vs cnt', path=f'Figures/{col}_vs_cnt.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uhrYzS2vd4s"
   },
   "source": [
    "The analysis of the above plots reveals the following insights:  \n",
    "- Over the two-year period, the average value of `cnt` has shown an upward trend. Since there is no real added value in this column since we are working over just two years, this column will be dropped  \n",
    "- The monthly and weekly graphs demonstrate a distinct peak in `cnt` during the summer months. Since in both of the columns the same trends can be observed, the month column will be dropped.\n",
    "- The day-of-the-month graph does not exhibit a clear correlation. Therefore, this column will be dropped.\n",
    "- The hour-of-the-day graph shows pronounced peaks during the morning and evening hours.  \n",
    "- The day-of-the-week graph indicates noticeable peaks on the fourth and fifth days of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JHsbq1Mvd4w"
   },
   "source": [
    "## 2.6 Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRH2D94Tvd4w"
   },
   "source": [
    "To assess whether the dataset exhibits stationarity, we will perform the Augmented Dickey-Fuller (ADF) test. This statistical test evaluates the null hypothesis ($H_0$) that the data contains a unit root, indicating non-stationarity. Rejection of the null hypothesis suggests that the data is stationary.\n",
    "\n",
    "**Hypothesis:**\n",
    "\n",
    "- $H_0$: The data contains a unit root and is non-stationary.\n",
    "- $H_1$: The data does not contain a unit root and is stationary.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "The outcome of the ADF test includes:\n",
    "- The test statistic, which is compared against critical values at various significance levels (e.g., 1%, 5%, 10%).\n",
    "- The p-value, indicating the probability of observing the test statistic under the null hypothesis.\n",
    "\n",
    "Based on these results, we will determine if stationarity can be assumed for the dataset or if additional transformations (e.g., differencing) are necessary to achieve stationarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxBJC2QSvd4x"
   },
   "outputs": [],
   "source": [
    "stat_tests = fc.StatisticalTests(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGu0U-Cmvd4x",
    "outputId": "f56d9838-4d9e-4125-d723-711e58084109"
   },
   "outputs": [],
   "source": [
    "stat_tests.stationary_test('cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qw5e8w0Gvd4x"
   },
   "source": [
    "## 2.7 Fourier analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fourier Transform (FT) in Time Series Analysis**\n",
    "\n",
    "A Fourier Transform (FT) converts data from the time domain into the frequency domain *(Omar, 2021)*. This transformation is specifically applicable to periodic signals in a time series format. When multiple periodic signals are combined, it can become challenging to discern where each signal begins and ends. By applying an FT, a frequency-amplitude graph is generated, allowing these components to be clearly identified.\n",
    "\n",
    "\n",
    "**Visualizing Periodic Signals with Inverse Fourier Transform (IFT)**\n",
    "\n",
    "To observe the periodic signals in their original form, an Inverse Fourier Transform (IFT) can be applied. However, before performing the IFT, the Fourier-transformed data must be cleaned to avoid merely reproducing the original time-domain data. This concept is demonstrated through the following visualizations:\n",
    "\n",
    "1. **Periodic Components**  \n",
    "   ![Periodic components](Figures/Explanations/Periodic%20components.png)  \n",
    "   *This figure visualizes the three individual components that constitute the data.*\n",
    "\n",
    "2. **Combined Data and Fourier Transform**  \n",
    "   ![Combined + decomposed](Figures/Explanations/Combined%20+%20decomp.png)  \n",
    "   *This figure shows the combined data alongside its Fourier Transform, highlighting three peaks at frequencies 10, 120, and 360.*\n",
    "\n",
    "3. **Low-Pass Filter**  \n",
    "   ![Low pass filter](Figures/Explanations/Low%20pass%20filter.png)  \n",
    "   *A low-pass filter is applied here to retain only the low-frequency signals.*\n",
    "\n",
    "4. **High-Pass Filter**  \n",
    "   ![High pass filter](Figures/Explanations/High%20pass%20filter.png)  \n",
    "   *A high-pass filter is applied to remove low-frequency signals, preserving only the high-frequency components.*\n",
    "\n",
    "5. **Bandstop Filter**  \n",
    "   ![Banstop filter](Figures/Explanations/Bandstop%20filter.png)  \n",
    "   *A bandstop filter is applied, filtering out medium-frequency signals while keeping the low and high-frequency components.*\n",
    "\n",
    "6. **Bandpass Filter**  \n",
    "   ![Bandpass filter](Figures/Explanations/Bandpass%20filter.png)  \n",
    "   *A bandpass filter is applied, retaining only the medium-frequency signals.*\n",
    "\n",
    "7. **Noisy Periodic Components**  \n",
    "   ![Noise periodic components](Figures/Explanations/Noise%20periodic%20components.png)  \n",
    "   *This figure visualizes the periodic components that form the data, which include significant noise.*\n",
    "\n",
    "8. **Noisy Combined Data and Fourier Transform**  \n",
    "   ![Noise combined + decomposed](Figures/Explanations/Noise%20combined%20+%20decomp.png)  \n",
    "   *This figure shows the noisy combined data and its Fourier Transform. Many small peaks are visible, alongside two prominent peaks at frequencies 10 and 120.*\n",
    "\n",
    "9. **Noise Filter**  \n",
    "   ![Noise filter](Figures/Explanations/Noise%20filter.png)  \n",
    "   *A noise filter is applied to remove low-amplitude peaks, ensuring only significant periodic components are retained.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "collapsed": true,
    "id": "p3XO1xxwvd4x",
    "outputId": "c1733e8b-4f95-4a7f-982d-0dcd0b394bf6"
   },
   "outputs": [],
   "source": [
    "stat_tests.fourier_analysis('cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAlVtmqBvd4x"
   },
   "source": [
    "The Fourier analysis reveals two prominent frequency spikes:\n",
    "\n",
    "1. A spike at a frequency of approximately **0.0001**, which corresponds to a periodicity of roughly **one year**.  \n",
    "2. A second spike at a frequency of approximately **0.041**, which translates to a periodicity of approximately **24 hours**.\n",
    "\n",
    "These findings suggest the presence of annual and daily patterns in the dataset, which may be significant for time series modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1Tc7USJvd4y"
   },
   "source": [
    "## 2.8 Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autocorrelation**\n",
    "\n",
    "Autocorrelation represents the similarity between a time series and a lagged version of itself. It measures the relationship between the current value of a variable and its past values. The scale for autocorrelation is the same as for regular correlation:  \n",
    "- **+1** indicates a perfect positive correlation,  \n",
    "- **-1** indicates a perfect negative correlation, and  \n",
    "- **0** indicates no correlation.  \n",
    "*(Smith, 2024)*  \n",
    "\n",
    "**Lagging**\n",
    "\n",
    "Lagging refers to shifting the values of a variable backward or forward in time to create new features, known as lagged features. These lagged features capture temporal dependencies and trends in the data, which can enhance the accuracy of predictive models.  \n",
    "*(\"Analyzing the Impact of Lagged Features in Time Series Forecasting: A Linear Regression Approach,\" 2024)*  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "id": "GnPEX7jcvd4y",
    "outputId": "4a51af72-412c-4ff5-f254-cd87113c03e9"
   },
   "outputs": [],
   "source": [
    "plot_acf(train_data['cnt'], lags=12, ax=plt.gca(), alpha=0.05)\n",
    "plt.savefig('Figures/ACF.png')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(train_data['cnt'], lags=12, ax=plt.gca(), alpha=0.05)\n",
    "plt.savefig('Figures/PACF.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uKnLqDY1Xze"
   },
   "source": [
    "Based on the combined autocorrelation and partial autocorrelation plots, we observe significant correlations up to **lag 5**. This indicates that past values within this lag range have a meaningful relationship with the current value, which may be important for time series modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8L4AhhTvd4s"
   },
   "source": [
    "## 2.9 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IycUN6gDvd4s"
   },
   "source": [
    "The exploratory data analysis (EDA) has provided valuable insights into the dataset, its structure, and the relationships between features. Based on the findings, the following data preprocessing steps will be applied to prepare the dataset for further analysis and modeling:\n",
    "\n",
    "1. **Column Dropping**:\n",
    "   - The following columns will be removed as they either lack meaningful contribution, exhibit high correlation with other features, or show redundant information:\n",
    "     - `holiday`: Weak correlation with the target variable and limited predictive power.\n",
    "     - `year`, `month`, `day_of_week`, `day`: These columns demonstrate trends or patterns already captured by other features, such as `hour` or aggregated time-series patterns.\n",
    "     - `atemp`: Highly correlated with `temp` (0.99), making it redundant.\n",
    "     - `windspeed`: Weak correlation with the target variable, indicating limited linear influence.\n",
    "\n",
    "2. **Dummy Variable Creation**:\n",
    "   - Dummy variables will be created for the `weathersit` column to capture its categorical nature effectively and ensure its compatibility with predictive modeling.\n",
    "\n",
    "3. **Feature Engineering with Fourier Analysis**:\n",
    "   - Fourier waves will be generated based on the following columns to capture their periodicity:\n",
    "     - `week` (annual periodicity).\n",
    "     - `hour` (daily periodicity).\n",
    "   - After generating the Fourier waves, the original `week` and `hour` columns will be dropped.\n",
    "\n",
    "4. **Indexing**:\n",
    "   - The `date_hour` column will be converted to the `datetime` format and set as the index for the dataset to facilitate time series analysis.\n",
    "\n",
    "These steps will ensure the dataset is optimized for modeling by retaining meaningful features, addressing redundancy, and incorporating temporal patterns effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8tx3QrBvd4t"
   },
   "source": [
    "## 2.10 Updating `test_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Time Series Components to Testing Data**\n",
    "\n",
    "During the Exploratory Data Analysis (EDA), time series-specific components were incorporated into the training data. To ensure that the models can generalize effectively, the testing data must match the structure of the training data. As a result, the same time series-specific components are now being added to the testing data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAvTG9o1vd4t"
   },
   "outputs": [],
   "source": [
    "test_data = fc.create_timeseries_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEzuQ2HFvd4w"
   },
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eB54kyTAvd4y"
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['holiday', 'year', 'month', 'day_of_week', 'day', 'atemp', 'windspeed']\n",
    "cols_to_dummy = ['weathersit']\n",
    "cols_to_fourier = ['hour', 'week']\n",
    "index_col = 'date_hour'\n",
    "\n",
    "fe = fc.FeatureEngineering(train_data, test_data, cols_to_drop, cols_to_dummy, cols_to_fourier, index_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Exploratory Data Analysis (EDA), the following decisions were made regarding dropping columns:\n",
    "\n",
    "- The columns **['holiday', 'windspeed']** were found to have very low correlation with the target variable and will therefore be dropped.\n",
    "- The time variables **['year', 'month', 'day_of_week', 'day']** were evaluated, and it was concluded that **'hour'** and **'week'** are the most relevant time-based features. Consequently, the other time variables will be dropped.\n",
    "- The feature **'atemp'** was removed due to its high correlation with **'temp'**, as retaining both could lead to multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.drop_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Creating dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Exploratory Data Analysis (EDA), it was observed that the **'weathersit'** column contains four categorical values. Since categories do not have a continuous influence on the target variable, **dummy variables** were created to represent these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.create_dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Creating fourier waves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Sinusoidal Features for Time-Based Variables**\n",
    "\n",
    "From the Fourier Transform analysis, we identified a clear daily repeating pattern in the **'cnt'** column. To capture this pattern, we created sinusoidal features using the **'hour'** column, generating both sine and cosine functions. These functions create a perfect wave with a minimum value of -1 and a maximum value of +1. Similarly, a clear yearly pattern was observed, prompting the same transformation for the **'week'** column.\n",
    "\n",
    "This transformation serves as a form of scaling, as both the **'hour'** and **'week'** columns now have a domain of $[-1, +1]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.fourier_wave()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Setting index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the time series is modeled correctly, the **datetime** column will be set as the index of the dataset. This allows the models to leverage the temporal structure of the data for accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = fe.set_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training data, a dummy column named **`weathersit_4`** has been created, but it is absent in the test data. Since the **`weathersit`** variable takes values ranging from 1 to 4, the absence of **`weathersit_4`** in the test data indicates that this category is not represented. \n",
    "\n",
    "To ensure consistency between the training and test datasets, we can safely drop the **`weathersit_4`** column from the training data without any loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('weathersit_4', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk83rfWHvd4y"
   },
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Regular models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**\n",
    "\n",
    "A **linear regression** model is an algorithm used to predict future outcomes based on a linear relationship between independent variables and a target variable.\n",
    "\n",
    "![LinReg best fit](Figures/Explanations/LinReg%20best%20fit.png)  \n",
    "*The figure above shows the best fit for a linear regression with only one independent variable (Kanade, 2023a).*\n",
    "\n",
    "The general formula for linear regression is:\n",
    "\n",
    "$$y(x) = p_0 + p_1x_1 + p_2x_2 + ... + p_{(n)}x_{(n)}$$\n",
    "\n",
    "Where:\n",
    "- **$y(x)$**: The predicted value for $y$ at moment $x$\n",
    "- **$p_0$**: The intersection with the y-axis, or the value for $y$ when $x$ is 0\n",
    "- **$p_t$**: The weights showing how much each variable contributes to the prediction\n",
    "- **$x_t$**: The variable\n",
    "- **$t$**: The number of variables\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "The **Root Mean Squared Error (RMSE)** or **Mean Squared Error (MSE)** score is typically used to assess the loss of the linear regression model. The output of the loss function is a single number representing the average squared difference between the actual and predicted values. The lower the number, the better the model's performance.\n",
    "\n",
    "The formula for MSE is:\n",
    "\n",
    "$$MSE = \\frac{1}{N}\\sum_{i = 1}^{n} (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "Where:\n",
    "- **$N$**: The number of observations\n",
    "- **$y_i$**: The actual value for $y$\n",
    "- **$\\hat{y_i}$**: The predicted value for $y$  \n",
    "- To calculate the RMSE, the square root of the MSE is taken\n",
    "\n",
    "*(Kanade, 2023b)*\n",
    "\n",
    "**Regularization**\n",
    "\n",
    "**Regularization** techniques are applied to prevent overfitting, which occurs when the model becomes too closely aligned with the training data and performs poorly on new data. Regularization introduces a penalty to the residual sum of squares (RSS), discouraging the use of excessively large coefficients. The formula for the RSS is as follows:\n",
    "\n",
    "$$ RSS = \\sum_{i = 1}^{n} (y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_jx_{ij}))^2 $$\n",
    "\n",
    "Where:\n",
    "- **$n$**: The total number of observations\n",
    "- **$y_i$**: The actual value for $y$\n",
    "- **$p$**: The total number of features\n",
    "- **$\\beta_j$**: The model's coefficients\n",
    "- **$x_{ij}$**: The $i^{th}$ observation for the $j^{th}$ feature\n",
    "- **$\\beta_0 + \\sum_{j = 1}^{p} \\beta_jx_{ij}$**: The predicted output for each observation\n",
    "\n",
    "**Types of Regularization**\n",
    "\n",
    "- **L1 Regularization (Lasso)**:  \n",
    "    Encourages sparsity by driving some coefficients to zero, thus performing feature selection.\n",
    "\n",
    "- **L2 Regularization (Ridge)**:  \n",
    "    Penalizes the squared values of coefficients, discouraging large coefficients and reducing model sensitivity to small changes in the input data.\n",
    "\n",
    "*(Ansari, 2023)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "param_grid = {'fit_intercept': [True, False], 'copy_X': [True, False]}\n",
    "\n",
    "gs = fc.GridSearch(train_data, test_data, target='cnt', model=lr, param_grid=param_grid, n_splits=5, order=1)\n",
    "gs.fit()\n",
    "gs.predict(test_data_pred_col)\n",
    "gs.to_csv(model='lr', path_add=f'order_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 KNN Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors (kNN) Regressor algorithm predicts a continuous target variable based on the average values of the nearest neighbors. It operates by plotting all data points in a multi-dimensional space, where each point is associated with a specific target value. When a new test data point is introduced, the algorithm computes the distances to the K nearest neighbors. \n",
    "\n",
    "**Prediction Methods**\n",
    "\n",
    "Once the distances to the K nearest neighbors are computed, the algorithm can make predictions using the following method:\n",
    "\n",
    "1. **Average of Nearest Neighbors**: The predicted value for the test data point is determined by averaging the target values of the K nearest neighbors. This method assumes that the value of the target variable for a test point is closely related to the values of its nearest neighbors.\n",
    "\n",
    "These methodologies ensure that the kNN Regressor algorithm provides a flexible and robust prediction mechanism for continuous datasets.\n",
    "\n",
    "**Importance of Standardization**\n",
    "\n",
    "The kNN Regressor uses distance to predict new values. This is effective as long as all features in the dataset have similar scales. If the features have different scales, those with larger values will dominate the distance metric. For example, if one feature is body height (measured in meters) and another is body weight (measured in kilograms), the distance metric will be skewed toward body weight unless both features are standardized. Without standardization, predictions can be heavily influenced by certain features, reducing the accuracy of the model.\n",
    "\n",
    "Therefore, standardization improves accuracy and prevents bias by ensuring all features contribute equally to the distance calculation.\n",
    "\n",
    "**Regularization in k-Nearest Neighbors (kNN Regressor)**\n",
    "\n",
    "In machine learning, **regularization** is used to prevent overfitting by adding a penalty to complex models. However, for non-parametric models like k-Nearest Neighbors (kNN), regularization is handled differently since KNN does not fit a parametric function. Instead, it makes predictions based on the proximity of data points in the feature space. While KNN does not explicitly include regularization terms like in parametric models, certain parameters control model complexity and improve generalization.\n",
    "\n",
    "**Key Regularization Aspects in KNN**\n",
    "\n",
    "1. **Choice of K (Number of Neighbors)**:\n",
    "   - The value of $K$ is crucial in controlling the **bias-variance trade-off**. A lower $K$ makes the model more sensitive to noise, as it focuses on fewer neighbors, leading to high variance and potential overfitting. A higher $K$ smooths the predictions, reducing variance but increasing bias, which can help prevent overfitting to training data.\n",
    "   - Therefore, selecting an optimal $K$ acts as an implicit form of regularization, with smaller values favoring higher complexity and larger values enforcing smoother, generalized predictions.\n",
    "\n",
    "**Regularization by Cross-Validation**\n",
    "- **Cross-validation** can be used in conjunction with the choice of $K$ to find the optimal balance, effectively acting as a regularization process. By tuning $K$ to minimize overfitting and maximize predictive performance, cross-validation helps ensure generalization of the model.\n",
    "\n",
    "In summary, while kNN does not explicitly apply regularization terms like in parametric models, the selection of $K$, feature scaling, and cross-validation all contribute to regularizing the model, controlling complexity, and improving generalization performance in kNN regression *(Baladram, 2024a)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "param_grid = {'n_neighbors': [1, 2, 3, 4, 5], 'weights': ['uniform', 'distance']}\n",
    "\n",
    "gs = fc.GridSearch(train_data, test_data, target='cnt', model=knn, param_grid=param_grid, n_splits=5, order=None)\n",
    "gs.fit()\n",
    "gs.predict(test_data_pred_col)\n",
    "gs.to_csv(model='knn', path_add=f'order_None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decision Tree Regressor is a non-parametric model used for regression tasks. It makes predictions by recursively partitioning the data into smaller subsets based on feature values. Each internal node in the tree corresponds to a decision made based on a specific feature, while each leaf node represents a predicted continuous outcome.\n",
    "\n",
    "**Splitting Criteria**\n",
    "\n",
    "The effectiveness of a Decision Tree Regressor hinges on the criteria used to determine the best splits. The most prevalent splitting criteria are:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**:  \n",
    "   MSE quantifies the variance of the target variable within the node. When splitting a node, the algorithm chooses the feature and threshold that minimizes the MSE in the resulting child nodes. Lower MSE values indicate a better split, as they lead to purer nodes with smaller variance in the target variable.\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**:  \n",
    "   MAE measures the average absolute differences between the predicted values and the actual target values. Like MSE, the algorithm seeks to minimize the MAE in the child nodes after a split, aiming for more accurate predictions.\n",
    "\n",
    "**Pruning and Overfitting**\n",
    "\n",
    "**Pruning** is a critical technique used to simplify the Decision Tree model by removing sections of the tree that do not contribute significantly to predictive power. This practice helps mitigate the risk of overfitting. Pruning can be categorized into two primary forms:\n",
    "\n",
    "1. **Pre-Pruning (Early Stopping)**:  \n",
    "   Pre-pruning involves halting the growth of the tree before it reaches its maximum depth. This can be achieved by imposing constraints such as limiting the depth of the tree, setting a minimum number of samples required to perform a split, or requiring a minimum number of samples at a leaf node. These measures prevent the model from becoming overly complex and prone to overfitting.\n",
    "\n",
    "2. **Post-Pruning**:  \n",
    "   In this approach, the tree is fully grown before assessing its structure. Post-pruning entails the removal of branches that contribute minimally to the model's performance. This is accomplished by evaluating the tree's performance on validation data and eliminating branches that do not enhance generalization.\n",
    "\n",
    "*Considerations for Pruning*\n",
    "\n",
    "While pruning is effective for reducing overfitting, it is crucial to avoid excessive pruning, as this can lead to **underfitting**. Underfitting occurs when the tree is overly simplistic and fails to capture the underlying patterns in the data, thereby impairing the modelâ€™s ability to make accurate predictions *(Baladram, 2024b)*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "param_grid = {'max_depth': [1, 2, 3, 4, 5], 'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20], 'min_samples_leaf': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "gs = fc.GridSearch(train_data, test_data, target='cnt', model=dt, param_grid=param_grid, n_splits=5, order=1)\n",
    "gs.fit()\n",
    "gs.predict(test_data_pred_col)\n",
    "gs.to_csv(model='dt', path_add=f'order_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid orange; background-color: #ffd7b3; color: #ff7b00; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> Check description\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest (RF) Regressor is an ensemble learning technique that constructs multiple decision trees and aggregates their individual predictions to create a more robust model, referred to as a \"forest.\" The primary method used to build each decision tree in a Random Forest is known as **bagging** (Bootstrap Aggregating).\n",
    "\n",
    "**Bagging Process**\n",
    "\n",
    "In the context of a Random Forest Regressor, bagging involves training each decision tree independently using a random sample of the training dataset. This sampling is done with replacement, allowing some observations to be included multiple times, while others may be excluded. The final prediction is made by averaging the predictions from all individual trees, which helps to reduce variance and improve accuracy.\n",
    "\n",
    "*Feature Randomization*\n",
    "\n",
    "In addition to random sampling of the data, Random Forest enhances the diversity of the individual trees by randomly selecting a subset of features to consider at each split in the decision tree. This randomness in feature selection reduces the correlation among the trees, which improves the model's accuracy and generalization ability. The ability to use different subsets of features in each tree helps Random Forest to effectively handle complex data with many features.\n",
    "\n",
    "The combination of bagging and feature randomization makes Random Forest a powerful and versatile machine learning algorithm, capable of effectively handling large datasets and complex relationships between features.\n",
    "\n",
    "**Training Time**\n",
    "\n",
    "The training time for a Random Forest Regressor is relatively short because the individual trees can be trained in parallel. Since each tree is trained independently, it is possible to leverage multi-core processors to train the trees simultaneously, further speeding up the process (Beheshti, 2022).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "param_grid = {'n_estimators': [25, 50, 75, 100, 150, 200], 'max_depth': [1, 2, 3, 4, 5],'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20], 'min_samples_leaf': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "gs = fc.GridSearch(train_data, test_data, target='cnt', model=rf, param_grid=param_grid, n_splits=5, order=1)\n",
    "gs.fit()\n",
    "gs.predict(test_data_pred_col)\n",
    "gs.to_csv(model='rf', path_add=f'order_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 XGB Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid red; background-color: #f8d7da; color: #721c24; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> ADD DESCRIPTION!!!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "param_grid = {'n_estimators': [25, 50, 75, 100, 150, 200], 'max_depth': [1, 2, 3, 4, 5], 'learning_rate': [0.01, 0.1, 0.3, 0.5, 1.0], 'subsample': [0.01, 0.1, 0.3, 0.5, 0.7, 1], 'colsample_bytree': [0.01, 0.1, 0.3, 0.5, 0.7, 1]}\n",
    "\n",
    "gs = fc.GridSearch(train_data, test_data, target='cnt', model=xgb, param_grid=param_grid, n_splits=5, order=1)\n",
    "gs.fit()\n",
    "gs.predict(test_data_pred_col)\n",
    "gs.to_csv(model='xgb', path_add=f'order_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the initial evaluation of all models:  \n",
    "\n",
    "- **Order 1 and Order 2** models performed significantly better than the model with no order.  \n",
    "- The difference in performance between **Order 1** and **Order 2** models was negligible.  \n",
    "- For the **KNN model**, however, the model with **no order** yielded the best performance.  \n",
    "\n",
    "As a result, we will proceed with **Order 1** for all subsequent analyses to maintain simplicity without sacrificing accuracy, except for the **KNN model**, where we will use **no order**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Timeseries models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid red; background-color: #f8d7da; color: #721c24; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> ADD DESCRIPTION!!!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data['cnt']\n",
    "\n",
    "param_grid = {'order': [(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, 0, 1), (2, 0, 0), (2, 0, 1)],\n",
    "              'seasonal_order': [(0, 0, 0, 24), (0, 0, 1, 24), (1, 0, 0, 24), (1, 0, 1, 24), (2, 0, 0, 24), (2, 0, 1, 24)]}\n",
    "\n",
    "sarimax_model = fc.SARIMAXModel(train_data, test_data, param_grid)\n",
    "sarimax_model.grid_search()\n",
    "\n",
    "sarimax_model.predict(test_data_pred_col)\n",
    "\n",
    "sarimax_model.save_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid red; background-color: #f8d7da; color: #721c24; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> ADD DESCRIPTION!!!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'seasonality_mode': ['additive', 'multiplicative'],\n",
    "              'changepoint_prior_scale': [0.01, 0.05, 0.1],\n",
    "              'yearly_seasonality': ['auto', True, False],\n",
    "              'weekly_seasonality': ['auto', True, False],\n",
    "              'daily_seasonality': ['auto', True, False]}\n",
    "\n",
    "prophet_model = fc.ProphetModel(train_data, test_data, param_grid=param_grid)\n",
    "prophet_model.grid_search()\n",
    "\n",
    "prophet_model.predict()\n",
    "\n",
    "prophet_model.save_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Hybrid model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid red; background-color: #f8d7da; color: #721c24; padding: 10px; border-radius: 5px; display: inline-block; max-width: 97%;\">\n",
    "    <strong>Warning:</strong> ADD DESCRIPTION!!!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = fc.HybridModel(train_data, test_data, 'cnt', {'lr': LinearRegression(), 'dt': DecisionTreeRegressor()})\n",
    "hm.fit()\n",
    "hm.predict(test_data_pred_col)\n",
    "hm.save_predictions()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
